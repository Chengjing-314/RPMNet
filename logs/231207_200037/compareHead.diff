diff --git a/.gitignore b/.gitignore
index a2a3d66..930b046 100644
--- a/.gitignore
+++ b/.gitignore
@@ -3,6 +3,7 @@
 
 ### Linux ###
 *~
+ycb/*
 
 # temporary files which can be created if a process still has a handle open of a deleted file
 .fuse_hidden*
diff --git a/augmentation_pipieline/augmentation_util.py b/augmentation_pipieline/augmentation_util.py
deleted file mode 100644
index 9d66606..0000000
--- a/augmentation_pipieline/augmentation_util.py
+++ /dev/null
@@ -1,87 +0,0 @@
-import torch 
-import numpy as np
-import random
-import open3d as o3d
-from tqdm import tqdm
-
-
-def load_pcd(path):
-    pcd = o3d.io.read_point_cloud(path)
-    pcd = np.asarray(pcd.points)
-    return pcd
-
-
-def load_pcds(paths):
-    pcds = []
-    for path in paths:
-        pcd = o3d.io.read_point_cloud(path)
-        pcd = np.asarray(pcd.points)
-        pcds.append(pcd)
-    return pcds
-
-def downsample(pcd:np.ndarray, voxel_size:float):
-    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd))
-    pcd = pcd.voxel_down_sample(voxel_size)
-    pcd = np.asarray(pcd.points)
-    return pcd
-
-
-def random_rotate(pcd:np.ndarray, angle_range:tuple):
-    axis = np.random.uniform(-1, 1, size=3)
-    angle = np.random.uniform(angle_range[0], angle_range[1])
-    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd))
-    pcd = pcd.rotate(axis, angle)
-    pcd = np.asarray(pcd.points)
-    return pcd, axis, angle
-
-def random_translate(pcd, translate_range):
-    translate = np.random.uniform(translate_range[0], translate_range[1], size=3)
-    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd))
-    pcd = pcd.translate(translate)
-    pcd = np.asarray(pcd.points)
-    return pcd, translate
-
-def random_chop(pcd, chop_range):
-    chop = np.random.uniform(chop_range[0], chop_range[1], size=3)
-    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd))
-    pcd = pcd.crop(chop)
-    pcd = np.asarray(pcd.points)
-    return pcd, chop
-
-def plane_cut(pcd):
-    
-    plane_normal = np.random.normal(size=3)
-    plane_normal = plane_normal / np.linalg.norm(plane_normal)
-    
-    indices = np.where(np.dot(pcd, plane_normal) < 0)[0]
-    
-    points = pcd[indices]
-    
-    return points
-
-
-def gaussian_noise(pcd, sigma):
-    noise = np.random.normal(0, sigma, size=pcd.shape)
-    pcd = pcd + noise
-    return pcd
-    
-
-def single_pcd_pipeline(file_path, num_transformation):
-    pcd = load_pcd(file_path)
-    pcds = []
-    axis = []
-    angle = []
-    translate = []
-    for _ in tqdm(range(num_transformation)):
-        downsampled_pcd = downsample(pcd, 0.05)
-        noisy_pcd = gaussian_noise(downsampled_pcd, 0.01)
-        cut_pcd = plane_cut(noisy_pcd)
-        rotated_pcd, axis_, angle_ = random_rotate(cut_pcd, (-np.pi, np.pi))
-        translated_pcd, translate_ = random_translate(rotated_pcd, (-0.2, 0.2))
-        pcds.append(translated_pcd)
-        axis.append(axis_)
-        angle.append(angle_)
-        translate.append(translate_)
-        
-    return pcds, axis, angle, translate
-
diff --git a/src/arguments.py b/src/arguments.py
index 729b842..8c5e04a 100644
--- a/src/arguments.py
+++ b/src/arguments.py
@@ -17,8 +17,8 @@ def rpmnet_arguments():
                         default='../datasets/modelnet40_ply_hdf5_2048',
                         type=str, metavar='PATH',
                         help='path to the processed dataset. Default: ../datasets/modelnet40_ply_hdf5_2048')
-    parser.add_argument('--dataset_type', default='modelnet_hdf',
-                        choices=['modelnet_hdf', 'bunny', 'armadillo', 'buddha', 'dragon'],
+    parser.add_argument('--dataset_type', default='ycb',
+                        choices=['modelnet_hdf', 'bunny', 'armadillo', 'buddha', 'dragon', 'ycb'],
                         metavar='DATASET', help='dataset type (default: modelnet_hdf)')
     parser.add_argument('--num_points', default=1024, type=int,
                         metavar='N', help='points in point-cloud (default: 1024)')
diff --git a/src/data_loader/datasets.py b/src/data_loader/datasets.py
index db98adb..113785b 100644
--- a/src/data_loader/datasets.py
+++ b/src/data_loader/datasets.py
@@ -4,6 +4,8 @@ import argparse
 import logging
 import os
 from typing import List
+import sys 
+sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
 
 import h5py
 import numpy as np
@@ -14,6 +16,7 @@ import torchvision
 import data_loader.transforms as Transforms
 import common.math.se3 as se3
 
+
 _logger = logging.getLogger()
 
 
@@ -147,44 +150,37 @@ def get_transforms(noise_type: str,
 
 class YCBobjects(Dataset):
     
-    def __init__(self, dataset_path: str, transform=None):
-        
+    def __init__(self, dataset_path: str, objects_list, transform=None):
         self._logger = logging.getLogger(self.__class__.__name__)
         self._root = dataset_path
         self._data = []
-        self._labels = []
         self._transform = transform
+        self.objects_list = objects_list
         
-        for root, dirs, files in os.walk(dataset_path):
-            for file in files:
-                if file.endswith(".pcd"):
-                    self._data.append(os.path.join(root, file))
-                    self._labels.append(root.split('/')[-1])
-        
-        self._logger.info('Loaded {} instances.'.format(len(self._data)))
+        # Iterate over the list of objects and add the path to the point cloud file to _data
+        for obj in self.objects_list:
+            ply_file = os.path.join(self._root, obj, 'clouds', 'merged_cloud.ply')
+            if os.path.exists(ply_file):
+                self._data.append(ply_file)
+            else:
+                self._logger.warning(f"File not found: {ply_file}")
         
-    
     def __getitem__(self, item):
-        
-        
+        # Load the point cloud
         ply = o3d.io.read_point_cloud(self._data[item])
         points = np.asarray(ply.points)
         
-        sample = {'points':points}
-        
+        sample = {'points': points}
         
+        # Apply transformations if any
         if self._transform:
             sample = self._transform(sample)
         
         return sample
     
-    
     def __len__(self):
         return len(self._data)
     
-    
-        
-        
 
 class ModelNetHdf(Dataset):
     def __init__(self, dataset_path: str, subset: str = 'train', categories: List = None, transform=None):
@@ -281,3 +277,19 @@ class ModelNetHdf(Dataset):
 
     def to_category(self, i):
         return self._idx2category[i]
+
+
+def main():
+    
+    output_directory = "/home/chengjing/Desktop/RPMNet/ycb"
+    f = open(os.path.join(output_directory, 'objects.txt'))
+    object_list = [line.strip() for line in f.readlines()]
+    f.close()
+    
+    dset = YCBobjects(output_directory, object_list)
+    
+    print("len(dset)", len(dset))
+
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
